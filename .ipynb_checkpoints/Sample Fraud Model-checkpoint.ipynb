{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5655b7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from elasticsearch import Elasticsearch\n",
    "import json\n",
    "import pyodbc\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "import textwrap\n",
    "import re\n",
    "import math\n",
    "\n",
    "today = date.today()\n",
    "\n",
    "def monthly_dif(d1, d2):\n",
    "    return (d1.year - d2.year) * 12 + d1.month - d2.month\n",
    "\n",
    "def numofdays(date1, date2):\n",
    "    return (date2-date1).days\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b05a326",
   "metadata": {},
   "outputs": [],
   "source": [
    "sospecha = pd.read_excel(\"file.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc6430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merchants = pd.DataFrame(data)\n",
    "df_transactions = pd.DataFrame(salida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c57da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trbd = df_transactions.sort_values(by=['id','id_transaccion']).reset_index(drop=True)\n",
    "\n",
    "#creación de variables principales de transacciones\n",
    "trbd = trbd.assign(fecha_trans = pd.to_datetime(trbd['fecha']).dt.date\n",
    "                   ,monto_aprobado = np.where(trbd['estado'] == 'Aprobada',trbd['monto'],0)\n",
    "                   ,flag_aprobado = np.where(trbd['estado'] == 'Aprobada',1,0)\n",
    "                   ,monto_denegado = np.where(trbd['estado'] == 'Denegada',trbd['monto'],0)\n",
    "                   ,flag_denegado = np.where(trbd['estado'] == 'Denegada',1,0)\n",
    "                   ,bin = trbd['bin'].replace(\" \", \"\").str[:6]\n",
    "                   ,hora = pd.to_datetime(trbd['fecha']).dt.hour\n",
    "                   ,geo = np.where(trbd['geo'].notnull(),'geo','no_geo')\n",
    "                   ,metodo = np.where(trbd['tipo'].isin([\"a\",\"b\"]),'c','d')\n",
    "                   ,bin_tras = trbd['bin'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1)\n",
    "                   ,one = 1\n",
    "                  ).reset_index(drop=True)\n",
    "\n",
    "#acumulado de aprobaciones y rechazos para porcentaje\n",
    "trbd = trbd.assign(cum_sum_aprobado = trbd.groupby(['id'])['monto_aprobado'].cumsum()\n",
    "                   ,cum_count_aprobado = trbd.groupby(['id'])['flag_aprobado'].cumsum()\n",
    "                   ,cum_sum_denegado = trbd.groupby(['id'])['monto_denegado'].cumsum()\n",
    "                   ,cum_count_denegado = trbd.groupby(['id'])['flag_denegado'].cumsum()\n",
    "                   )\n",
    "\n",
    "#porcentaje de aprobación por transacción y cantidad de tcs dentro del mismo comercio\n",
    "trbd = trbd.assign(pct_monto_aprobado = trbd.cum_sum_aprobado/(trbd.cum_sum_aprobado + trbd.cum_sum_denegado)\n",
    "                   ,pct_count_aprobado = trbd.cum_count_aprobado/(trbd.cum_count_aprobado + trbd.cum_count_denegado)\n",
    "                   ,cant_tc = trbd.groupby(['id','bin_tras'])['one'].cumsum()\n",
    "                   ,transaction_num = trbd.groupby(['id'])['one'].cumsum()\n",
    "                  )\n",
    "\n",
    "trbd = trbd.assign(pct_monto_ap_group = np.where((trbd.pct_monto_aprobado >= 0) & (trbd.pct_monto_aprobado < 0.1),'ap_10',\n",
    "                                                 np.where((trbd.pct_monto_aprobado >= 0.1) & (trbd.pct_monto_aprobado < 0.2),'ap_20',\n",
    "                                                          np.where((trbd.pct_monto_aprobado >= 0.2) & (trbd.pct_monto_aprobado < 0.4),'ap_40',\n",
    "                                                                   np.where((trbd.pct_monto_aprobado >= 0.4) & (trbd.pct_monto_aprobado < 0.6),'ap_60',\n",
    "                                                                            np.where((trbd.pct_monto_aprobado >= 0.6) & (trbd.pct_monto_aprobado < 0.8),'ap_80',\n",
    "                                                                                     np.where((trbd.pct_monto_aprobado >= 0.8),'ap_1'\n",
    "                                                                                              ,'ap_otro'))))))\n",
    "                   \n",
    "                   ,pct_count_ap_group = np.where((trbd.pct_count_aprobado >= 0) & (trbd.pct_count_aprobado < 0.1),'cap_10',\n",
    "                                                 np.where((trbd.pct_count_aprobado >= 0.1) & (trbd.pct_count_aprobado < 0.2),'cap_20',\n",
    "                                                          np.where((trbd.pct_count_aprobado >= 0.2) & (trbd.pct_count_aprobado < 0.4),'cap_40',\n",
    "                                                                   np.where((trbd.pct_count_aprobado >= 0.4) & (trbd.pct_count_aprobado < 0.6),'cap_60',\n",
    "                                                                            np.where((trbd.pct_count_aprobado >= 0.6) & (trbd.pct_count_aprobado < 0.8),'cap_80',\n",
    "                                                                                     np.where((trbd.pct_count_aprobado >= 0.8),'cap_1'\n",
    "                                                                                              ,'cap_otro'))))))\n",
    "                   ,cant_tc_group = np.where((trbd.cant_tc >= 0) & (trbd.cant_tc <= 0),'cant_0_1',\n",
    "                                                 np.where((trbd.cant_tc >= 1) & (trbd.cant_tc <= 2),'cant_1_2',\n",
    "                                                          np.where((trbd.cant_tc >= 3) & (trbd.cant_tc <= 5),'cant_3_5',\n",
    "                                                                   np.where((trbd.cant_tc >= 6) & (trbd.cant_tc <= 9),'cant_6_9',\n",
    "                                                                            np.where((trbd.cant_tc >= 10) & (trbd.cant_tc <= 15),'cant_10_15',\n",
    "                                                                                     np.where((trbd.cant_tc >= 16),'cant_inf'\n",
    "                                                                                              ,'cant_otro'))))))\n",
    "                   \n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96369e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "com00 = df_merchants.assign(fecha_crea = pd.to_datetime(df_merchants['fecha']).dt.date                              \n",
    "                              ,antiguedad = monthly_dif(today, pd.to_datetime(df_merchants['fecha']).dt)\n",
    "                              ,edad = np.where(df_merchants['birthdate'].notnull,(1/12)*monthly_dif(today, pd.to_datetime(df_merchants['birthdate']).dt),0).astype(int)\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d4f7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bd00 = trbd.merge(com00, on='id_comercio', how='left', indicator=False)\n",
    "bd00 = bd00.assign(mdi = monthly_dif(pd.to_datetime(bd00['fecha_trans']).dt,pd.to_datetime(bd00['fecha_crea']).dt)\n",
    "                  ,ddi = bd00['fecha_trans'] - bd00['fecha_crea']\n",
    "                  )\n",
    "\n",
    "#agrupación de mdi y ddi\n",
    "bd00 = bd00.assign(mdi_group = np.where(bd00['mdi'].isin([0]),'mdi_0',\n",
    "                                        np.where(bd00['mdi'].isin([1]),'mdi_1',\n",
    "                                                 np.where(bd00['mdi'].isin([2]),'mdi_2',\n",
    "                                                          np.where(bd00['mdi'].isin([3]),'mdi_3',\n",
    "                                                                   np.where(bd00['mdi'].isin([4]),'mdi_4',\n",
    "                                                                            np.where(bd00['mdi'].isin([5]),'mdi_5',\n",
    "                                                                                     np.where(bd00['mdi'].isin([6]),'mdi_6',\n",
    "                                                                                              np.where(bd00['mdi'].isin([7]),'mdi_7',\n",
    "                                                                                                       np.where(bd00['mdi'].isin([8]),'mdi_8',\n",
    "                                                                                                                np.where(bd00['mdi'].isin([9]),'mdi_9',\n",
    "                                                                                                                         np.where(bd00['mdi'].isin([10]),'mdi_10',\n",
    "                                                                                                                                  np.where(bd00['mdi'].isin([11]),'mdi_11',\n",
    "                                                                                                                                           np.where(bd00['mdi']>=12,'mdi_12',\n",
    "                                                                                                                                                    'mdi_otro')))))))))))))\n",
    "                  \n",
    "                  \n",
    "                  ,ddi_group = np.where((bd00['ddi']>='0 days') & (bd00['ddi']<'3 days'),'ddi_0_3',\n",
    "                                        np.where((bd00['ddi']>='3 days') & (bd00['ddi']<'7 days'),'ddi_3_7',\n",
    "                                                 np.where((bd00['ddi']>='7 days') & (bd00['ddi']<'14 days'),'ddi_7_14',\n",
    "                                                          np.where((bd00['ddi']>='14 days') & (bd00['ddi']<'21 days'),'ddi_14_21',\n",
    "                                                                   np.where((bd00['ddi']>='21 days') & (bd00['ddi']<'30 days'),'ddi_21_30',\n",
    "                                                                            np.where((bd00['ddi']>='30 days') & (bd00['ddi']<'45 days'),'ddi_30_45',\n",
    "                                                                                     np.where((bd00['ddi']>='45 days') & (bd00['ddi']<'60 days'),'ddi_45_60',\n",
    "                                                                                              np.where((bd00['ddi']>='60 days') & (bd00['ddi']<'90 days'),'ddi_60_90',\n",
    "                                                                                                       np.where((bd00['ddi']>='90 days') & (bd00['ddi']<'120 days'),'ddi_90_120',\n",
    "                                                                                                                np.where((bd00['ddi']>='120 days'),'ddi_120',\n",
    "                                                                                                                                                    'ddi_otro'))))))))))\n",
    "\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9c5590",
   "metadata": {},
   "outputs": [],
   "source": [
    "fecha_valuacion = today\n",
    "\n",
    "bd01 = bd00[(bd00.fecha_trans <= fecha_valuacion) & (bd00.id>=25)].reset_index(drop=True)\n",
    "bd01 = bd01.merge(sospecha, on='id_transaccion', how='left', indicator=False)\n",
    "bd01 = bd01.assign(flag_f = np.where(np.isnan(bd01.Flag_F), 0, bd01.Flag_F))\n",
    "\n",
    "#dummying categorical variables in model bd\n",
    "provincia_m = pd.get_dummies(bd01['location'],drop_first = False)\n",
    "hora_m = pd.get_dummies(bd01['hora'],drop_first = False)\n",
    "hora_m.rename(columns={1:'uno', 2:'dos',3:'tres',4:'cuatro',5:'cinco',6:'seis',7:'siete', 8:'ocho'\n",
    "                     ,9:'nueve',10:'diez',11:'once',12:'doce'\n",
    "                    ,13:'trece', 14:'catorce',15:'quince',16:'dieciseis',17:'diecisiete',18:'dieciocho',19:'diecinueve',20:'veinte'\n",
    "                     ,21:'veintiuno',22:'veintidos',23:'veintitres',0:'veinticuatro'}, inplace=True)\n",
    "estado_m = pd.get_dummies(bd01['estado'],drop_first = False)\n",
    "ddi_group_m = pd.get_dummies(bd01['ddi_group'],drop_first = False)\n",
    "mdi_group_m = pd.get_dummies(bd01['mdi_group'],drop_first = False)\n",
    "pct_monto_ap_group_m = pd.get_dummies(bd01['pct_monto_ap_group'],drop_first = False)\n",
    "pct_count_ap_group_m = pd.get_dummies(bd01['pct_count_ap_group'],drop_first = False)\n",
    "cant_tc_group_m = pd.get_dummies(bd01['cant_tc_group'],drop_first = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d69423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "y=bd01['flag_f'].reset_index(drop=True)\n",
    "cols = ['pct_monto_aprobado', 'pct_count_aprobado']\n",
    "\n",
    "x=pd.concat([pct_monto_ap_group_m.reset_index(drop=True)\n",
    "            ,pct_count_ap_group_m.reset_index(drop=True)\n",
    "            ,cant_tc_group_m.reset_index(drop=True)\n",
    "            ,location.reset_index(drop=True)\n",
    "            ,hora_m.reset_index(drop=True)\n",
    "            ,estado_m.reset_index(drop=True)\n",
    "            ,antiguedad_m.reset_index(drop=True)\n",
    "            ,ddi_group_m.reset_index(drop=True)\n",
    "            ,mdi_group_m.reset_index(drop=True)\n",
    "            ],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a4d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef06dd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=123)\n",
    "x_train_r = x_train.drop(labels='index',axis=1)\n",
    "x_test_r = x_test.drop(labels='index',axis=1)\n",
    "\n",
    "logreg.fit(x_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(x_test)\n",
    "y_pred_prob_x_train = logreg.predict_proba(x_train)\n",
    "y_pred_prob_x_test = logreg.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cde9b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gini, roc\n",
    "y_pred_proba = logreg.predict_proba(x_test)[::,1]\n",
    "fpr, tpr = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = round(metrics.roc_auc_score(y_test, y_pred_proba),4)\n",
    "gini = round((metrics.roc_auc_score(y_test, y_pred_proba))*2-1,4)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc)+\" gini=\"+str(gini))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eac5bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matriz de confusión\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb0d522",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(bd01['flag_f'])\n",
    "feature_list = list(x.columns)\n",
    "features = np.array(x)\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.30, random_state = 123)\n",
    "\n",
    "#Balance de 50/50\n",
    "balance0 = pd.concat([x_train,y_train],axis=1).reset_index(drop=True)\n",
    "train_0 = balance0[balance0.flag_f==0]\n",
    "train_1 = balance0[balance0.flag_f==1]\n",
    "train_0 = train_0.sample(n = len(train_1))\n",
    "x_rftrain = pd.concat([train_1,train_0])\n",
    "y_rftrain = x_rftrain['flag_f']\n",
    "x_rftrain = x_rftrain.drop(['flag_f'], axis=1)\n",
    "\n",
    "y_rftrain = np.array(y_rftrain)\n",
    "x_rftrain = np.array(x_rftrain)\n",
    "\n",
    "y_rftest = np.array(y_test)\n",
    "x_rftest = np.array(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffad0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators = 50, random_state = 123)\n",
    "rf.fit(x_rftrain, y_rftrain)\n",
    "rf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ca15b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rf.predict(x_rftest)\n",
    "predictions = rf.predict(test_features)\n",
    "\n",
    "errors = abs(predictions - y_rftest)\n",
    "errors = abs(predictions - test_labels)\n",
    "\n",
    "#matriz de confusión\n",
    "c_y_rftest = np.where(predictions < 0.45,0,1)\n",
    "cnf_matrix = metrics.confusion_matrix(pd.DataFrame(test_labels), pd.DataFrame(c_y_rftest))\n",
    "print(cnf_matrix)\n",
    "\n",
    "importances = list(rf.feature_importances_)\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
